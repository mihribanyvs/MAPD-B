{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3fc3226-cf93-4251-905b-18b6720a4dd1",
   "metadata": {},
   "source": [
    "# Lecture 3: Stream processing with Spark\n",
    "\n",
    "_Spark Structured Streaming_ is an extension of the Spark API that enables scalable stream processing.\n",
    "\n",
    "It's a well-documented API [link](https://spark.apache.org/docs/3.5.0/structured-streaming-programming-guide.html), which is however different from Spark Streaming, which has recently become deprecated since Spark version 3.4.0 (~last year)\n",
    "\n",
    "The main difference between Spark `Streaming` and `Structured Streaming` is that the former is based on the low-level RDD API, while the latter is based on DataFrames.\n",
    "\n",
    "Both APIs have however been designed to help with the continuous processing of streaming applications. \n",
    "\n",
    "The continuous stream of input data can be ingested from many data sources such as **Kafka**, **Amazon S3**, or **TCP sockets**. \n",
    "\n",
    "Processed data can be exported to an external database and used to make live dashboards or offline analyses, stored in files, or used in a further stage of a Kafka pipeline. \n",
    "\n",
    "Overall, the practice of reading data from a set of sources, pre-processing it, and then storing it in a different format for later analysis is extremely common and has its own name: **real-time ETL pipelines**.\n",
    "- **E**xtract\n",
    "- **T**ransform\n",
    "- **L**oad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2d05b-82b9-498d-99d9-17e263c58636",
   "metadata": {},
   "source": [
    "## Structured Streaming\n",
    "\n",
    "The key idea of this stream processing model is to treat the continuous stream as a table that is continuously appended to.\n",
    "\n",
    "This allows users to view the continuously incoming data as a DataFrame with new records being new rows to be included. It further expresses stream processing as a standard batch-like query on a static table, similar to what we have done in the Spark DataFrame lecture.\n",
    "\n",
    "![](imgs/lecture3/structured-streaming-stream-as-a-table.png)\n",
    "\n",
    "Internally, the stream is divided into micro-batches produced by a \"trigger,\" which could represent any given condition (e.g., every 1 second):\n",
    "\n",
    "- The input stream is a table (DataFrame) or the new rows to be appended to the previous DataFrame.\n",
    "- Every operation/query will produce a result table (DataFrame).\n",
    "- When the output table is updated, it can be written somewhere thanks to an output module/type.\n",
    "\n",
    "![](imgs/lecture3/structured-streaming-model.png)\n",
    "\n",
    "**NB:** Although the input table can be viewed as an always-growing DataFrame, **Spark does not actually materialize the entire table**. \n",
    "\n",
    "- Only the latest data are processed (latest batch) and then discarded.\n",
    "- Conversely, the result table can be updated to keep track of the results from previous batches.\n",
    "\n",
    "There are three output modes:\n",
    "\n",
    "- ***Complete Mode*** where the entire output table is written to the Sink.\n",
    "- ***Append Mode*** where only the new rows appended in the Result Table since the last trigger will be processed by the Sink.\n",
    "- ***Update Mode*** where only the rows that were updated in the result table since the last trigger will be written to the Sink.\n",
    "\n",
    "In this notebook, the *Update Mode* and *Console* output sink will be used.\n",
    "\n",
    "- Results (i.e., only the updated records) will be displayed on the screen (i.e., on the \"terminal\").\n",
    "- However, the Sink could be a database, a file, Kafka, or more, depending on the needs and applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2964c9a4-9552-4420-87b6-bcdaf29f03c1",
   "metadata": {},
   "source": [
    "## Create and Start a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce1d0bc-fea9-4d3d-bc81-f3914db66161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# build a SparkSession \n",
    "#   connect to the master node on the port where the master node is listening (7077)\n",
    "#   declare the app name \n",
    "#   configure the executor memory to 512 MB\n",
    "#   either *connect* or *create* a new Spark Context\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"My streaming spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af68f7-d201-480f-97f6-76cec4be859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a18ca8c-aeb0-4ff4-af13-5e98edec670f",
   "metadata": {},
   "source": [
    "### TCP Socket Source\n",
    "\n",
    "For this example, Spark will read data from a TCP socket using Spark Structured Streaming.\n",
    "\n",
    "A TCP socket is a communication endpoint used to establish a connection between two devices over a network. You can think of it as a telephone connection: two endpoints have to establish a connection; once the connection is established, communication can occur, with data transfer; as soon as one of the two ends interrupts the connection, the whole communication is lost. \n",
    "\n",
    "We will generate a dummy data stream representing fake credit card transactions.\n",
    "\n",
    "A simple Python program will be used to create this data stream. You will be able to find it in `utils/producer.py`. When executed, the producer will try to establish a TCP connection and send data on port `5555` of a given `host` (`spark-master` in our case). \n",
    "\n",
    "Before executing the producer program, take a moment to review the `producer.py` code to understand how it works. It's important to understand the logic of the program before using it to generate the streaming data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1d3ad-83d3-498e-a083-5a9674922f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat utils/producer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d606a75-c9ae-490c-a8f6-b264aa19e667",
   "metadata": {},
   "source": [
    "The producer will generate new records in the form of a random combination of:\n",
    "- `name`\n",
    "- `surname`\n",
    "- `amount`: amount of the credit card transaction\n",
    "- `delta_t`: time between transactions\n",
    "- `flag`: random flag to indicate if potentially fraudulent or not\n",
    "\n",
    "This information will be formatted into a `.json` data format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4829e-f168-4061-8e54-060e527e4224",
   "metadata": {},
   "source": [
    "### Creating the streaming DataFrame from a TCP socket source\n",
    "\n",
    "To inform Spark that the data source will be a TCP socket located at a specific `hostname` and `port`, we can use the options `host` and `port` methods.\n",
    "* When declaring this source a message appears, warning that the TCP source is only meant for testing purposes.\n",
    "\n",
    "Refer to the [documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html) for additional available options.\n",
    "\n",
    "Notice that the syntax is equivalent to the one used for example to read a set of files from a disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c7897-ee28-49b5-993d-a6b810dda907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hostname and port number\n",
    "hostname = \"spark-master\"\n",
    "portnumber = 5555\n",
    "\n",
    "rawMessagesDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", hostname)\n",
    "    .option(\"port\", portnumber)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c940de14-ecac-4b8e-b91a-ae7da20a812f",
   "metadata": {},
   "source": [
    "### Start the python producer.py script\n",
    "\n",
    "From a terminal/WSL, connect to the `spark-master` Docker container using the command\n",
    "```bash\n",
    "docker exec -it spark-master bash\n",
    "``` \n",
    "\n",
    "From inside the docker container, move to the `/mapd-workspace` folder and execute the python script with the option `--hostname spark-master`:\n",
    "\n",
    "```bash\n",
    "python /mapd-workspace/notebooks/utils/producer.py --hostname spark-master\n",
    "```\n",
    "\n",
    "The producer application will be automatically closed when the streaming application terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffcf8e-d85c-461e-ae10-42fd5dbb27bf",
   "metadata": {},
   "source": [
    "### Running the first streaming application\n",
    "\n",
    "The first streaming query will be a simple `show` of the DataFrame\n",
    "* No processing between `rawMessagesDf` and output sink (`writeStream`)\n",
    "\n",
    "Output mode is set to `update`, hence every new message received by the TCP source will be processed by the output sink.\n",
    "* Output format `console` indicates that the output will be displayed on the screen before being discarded\n",
    "\n",
    "The streaming query is triggered every 2 seconds\n",
    "* refer to the [documentation](https://spark.apache.org/docs/3.5.0/structured-streaming-programming-guide.html#streaming-table-apis) for additional trigger types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f4d0da-c4f9-4ee1-91c2-e3abdd4129aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    rawMessagesDf\n",
    "    .writeStream\n",
    "    .outputMode(\"update\")\n",
    "    .format(\"console\")\n",
    "    .trigger(processingTime='2 seconds')\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cbbca9-2887-4ef2-a81f-4a1672ec2c5d",
   "metadata": {},
   "source": [
    "Run this cell to stop the streaming query execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fd106d-9dbc-4911-b528-15389459baee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d467a2d9-bdaf-438c-818b-f12cf540f871",
   "metadata": {},
   "source": [
    "## Example of streaming query: data parsing\n",
    "\n",
    "Data received from the TCP source is seen as a `string` by the spark application\n",
    "\n",
    "We first must develop an application parsing the string and creating a column for each `json` field in order to start processing the dataset using the DataFrame API functionalities\n",
    "\n",
    "It can be useful to start with a set of test data, to develop this query.\n",
    "\n",
    "**The same Spark code (transformations/actions) used for processing a \"static\" DataFrame can be used for the streaming context!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441dc43a-5632-4921-9011-e04720a1e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy data for testing purposes\n",
    "testData = [\n",
    "    '{\"name\": \"Jill\", \"surname\": \"Millers\", \"amount\": 736.56, \"delta_t\": 7.78, \"flag\": 0}',\n",
    "    '{\"name\": \"John\", \"surname\": \"Johnson\", \"amount\": 986.47, \"delta_t\": 3.9, \"flag\": 0}',\n",
    "    '{\"name\": \"John\", \"surname\": \"Jones\", \"amount\": 249.9, \"delta_t\": 0.62, \"flag\": 1}',\n",
    "    '{\"name\": \"Andy\", \"surname\": \"Jones\", \"amount\": 950.95, \"delta_t\": 6.02, \"flag\": 0}',\n",
    "    '{\"name\": \"Jill\", \"surname\": \"Millers\", \"amount\": 724.32, \"delta_t\": 9.19, \"flag\": 0}',\n",
    "    '{\"name\": \"John\", \"surname\": \"Johnson\", \"amount\": 850.07, \"delta_t\": 7.33, \"flag\": 1}',\n",
    "    '{\"name\": \"Andy\", \"surname\": \"Smith\", \"amount\": 557.48, \"delta_t\": 9.64, \"flag\": 0}',\n",
    "    '{\"name\": \"Alice\", \"surname\": \"Darby\", \"amount\": 424.75, \"delta_t\": 7.76, \"flag\": 0}'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe33e7b1-fa87-454a-9ad7-41dea82d8f21",
   "metadata": {},
   "source": [
    "Create a Spark DataFrame by importing the `testData`.\n",
    "\n",
    "It can be useful to investigate the `pyspark.sql.types` to check whether there is any helping function that could be used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6935be-78f3-40a3-9fea-6f5a99f34570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# create a spark dataframe from testdata\n",
    "testDf = spark.createDataFrame(testData, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f34563-f844-4b85-b4df-36609dbc49ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the dataframe content\n",
    "testDf.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c77e8a-4160-4d34-818c-5adaf2143fa2",
   "metadata": {},
   "source": [
    "Data are now in the same format as the one received by the socket data source. \n",
    "\n",
    "The function `from_json` can be used to parse a `json` string with a given schema. Aa always, have a look at the [documentation](https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.functions.from_json.html?highlight=from_json) before using it.\n",
    "\n",
    "Start by defining a schema for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f0616-6cc0-4930-b78b-5e80073b0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, IntegerType\n",
    "\n",
    "# create the schema\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"name\", StringType()),\n",
    "        \"\"\"\n",
    "           ...\n",
    "           ...\n",
    "           ...\n",
    "        \"\"\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bc136d-00b9-4f75-a286-c1eb5969b5de",
   "metadata": {},
   "source": [
    "And create a \"parsed\" DataFrame to verify if our schema is properly addressing the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e104895-6f22-4908-a7ba-23c19c36f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the dataframe and show its structure\n",
    "parsedTestDf = testDf.select(f.from_json('value', schema=schema))\n",
    "\n",
    "parsedTestDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069d5bcb-92f5-4a88-b8fe-7b7173c580f0",
   "metadata": {},
   "source": [
    "The structure obtained using `from_json` is actually inherently nested...\n",
    "The `json` messages were correctly parsed, but a nested DataFrame is returned, which should be flattened before being able to use it as a \"plain DataFrame\".\n",
    "\n",
    "\n",
    "As discussed in the DataFrame notebook, this can be easily solved in a variety of ways, for instance by selecting the columns we are interested in using from now on...\n",
    "\n",
    "\n",
    "As an example, the `name` and `surname` columns could be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b4364-3c96-449e-a76f-34ecde596938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show values from the parsedTestDf\n",
    "parsedTestDf.show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cdd6ef-8c87-4380-83fa-491f005588e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create the parsedTestDf by \n",
    "# - aliasing the values produced by from_json as `data`\n",
    "# - selecting the columns `data.name` and `data.surname`\n",
    "parsedTestDf = (\n",
    "    testDf\n",
    "    .select(\"\"\"---\"\"\")\n",
    "    .select(\"\"\"---\"\"\") \n",
    ")\n",
    "\n",
    "# NB: data.* will select all the columns, as in plain SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f7b468-6872-4b20-8516-2dff853973f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the new schema\n",
    "parsedTestDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccbf50-2527-4f27-b1a5-55ff47b14f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and show the new content\n",
    "parsedTestDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77753449-fada-4d7d-bbd0-3e15c32deab2",
   "metadata": {},
   "source": [
    "### From static to streaming queries\n",
    "\n",
    "Now that we have developed a way to extract and interpret the data using a static example, we can reuse the very same code for the streaming query, by simply \"chaining\" the application to the source and directing its results to the sink.\n",
    "\n",
    "**NB**: Remember to restart the producer application before starting the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e33476-1b21-489c-be77-7ac35fa79f96",
   "metadata": {},
   "source": [
    "Re-create the \"raw\" DataFrame connecting the Spark Structured Streaming Context to the input TCP socket.\n",
    "\n",
    "**Do not `start` the stream just yet!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b089163-7791-42bc-8bb5-fec13a46a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hostname and port number\n",
    "hostname = \"spark-master\"\n",
    "portnumber = 5555\n",
    "\n",
    "# recreate the streaming raw dataframe\n",
    "rawMessagesDf = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"socket\")\n",
    "    .option(\"host\", hostname)\n",
    "    .option(\"port\", portnumber)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970f74b-7050-4c4d-9865-a8dac48a39c7",
   "metadata": {},
   "source": [
    "Starting from the streaming DataFrame, issue the appropriate transformations to interpret its `json` format, and prepare a new parsed DataFrame including all (`*`) the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0710c-bd63-4fae-9813-3be2994d8806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the json lines using the previous schema\n",
    "# select all columns \n",
    "parsedDf = (\n",
    "    rawMessagesDf.\"\"\"...\"\"\"\n",
    " \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d19c40-2ba2-4bde-83a0-6e1e2f20718d",
   "metadata": {},
   "source": [
    "Start the application as done previously, with:\n",
    "- `outputMode=update`\n",
    "- `format=console`\n",
    "\n",
    "Choose the trigger as you prefer. \n",
    "A suggestion is to keep it simple, and use a time-based trigger of 2 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9450a2-c90d-406e-964c-f80f671faeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    parsedDf\n",
    "    .writeStream\n",
    "    \"\"\"...\"\"\"\n",
    "    \"\"\"...\"\"\"\n",
    "    \"\"\"...\"\"\"\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115592f-7e2e-4cc7-a827-0ebbe2548cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7387302-1d48-46bb-82f8-b5c0cfc4124b",
   "metadata": {},
   "source": [
    "### Process each bach to identify possibly fraudolent transactions\n",
    "\n",
    "1. compute the _number of flagged transactions per batch per user_ (create a unique `userID` field as the combination of _FirstLastname_ to idenfity individual users)\n",
    "2. identify all the \"suspicios\" transactions per user: all users with more than one flagged transaction per batch will be assigned a `isFraud` boolean variable\n",
    "3. format the resulting `userID` and `isFraud` information in a DataFrame to mimick a \"live-report\" of the suspicious transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fa8b2c-93c4-4ec9-bc78-a23edec01c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hostname and port number\n",
    "hostname = \"spark-master\"\n",
    "portnumber = 5555\n",
    "\n",
    "rawMessagesDf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a5915-05e5-49ba-b33d-da3f04aa2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse json lines, use a schema\n",
    "parsedDf = (\n",
    "    rawMessagesDf.\n",
    "\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ed3f10-d171-42bf-919d-a5d6f67252b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find number of transactions for each user when flag = 1 \n",
    "#    declare a new column to create a unique user identifier \n",
    "#    this can be easily done by concatenating first- and last-name fields\n",
    "#    check the concat function from pyspark.sql.functions \n",
    "numTransactions = (\n",
    "    parsedDf\n",
    " \n",
    " \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344be81f-7865-482b-b4a5-c351714230ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find suspicious transactions\n",
    "#    filter only users with more than one transaction per batch\n",
    "#    create a \"fraud\" column with a value of 1 for the selected users (check the lit function)\n",
    "#    from the dataframe, project the unique id, fraud flag and number of transaction columns\n",
    "susTransactions = (\n",
    "    numTransactions\n",
    "\n",
    "\n",
    ")                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82987df-bce0-4544-aa92-fd47cfd84481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line is a trick to force Spark to use a small number of partitions (4 in this example)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f6c32-7767-4510-821f-91c793a4905c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    susTransactions\n",
    "    \"\"\"\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    \"\"\"\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0892665a-fb16-41fd-92a5-e9ef7baa11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381287d-287c-45e3-beb1-b51c74ddb4c8",
   "metadata": {},
   "source": [
    "Since this query contains aggregations, output mode can be changed to `complete`.\n",
    "In this case, the full set of results is displayed for every batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305d190c-6ff9-476b-8f05-21da7c39490e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    susTransactions\n",
    "    \"\"\"\n",
    "    ...\n",
    "    ...\n",
    "    ...\n",
    "    \"\"\"\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbeda4-f370-494e-a851-e0b5d2a84dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515d2d5-3403-4e8f-bfdf-7684c983608c",
   "metadata": {},
   "source": [
    "## Stop spark worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3bd53a-ae65-4288-89fa-81062e7f3e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34eb3b-f324-4ae6-a9e7-1954ef509e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

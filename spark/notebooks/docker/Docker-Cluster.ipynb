{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "785e8bfd",
   "metadata": {},
   "source": [
    "## Test notebook for the cluster created with docker compose.\n",
    "\n",
    "The dashboard of the Cluster is port mapped from the Spark master container, and should be visible on `localhost:8080`.\n",
    "\n",
    "The Spark master address inside the Docker network created once the `docker compose up` is executed is the following: `spark://spark-master:7077`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3d9c9",
   "metadata": {},
   "source": [
    "### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed0d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the python libraries to create/connect to a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# from the CLIENT, create a Spark Session\n",
    "# pointing to the Spark master address\n",
    "#\n",
    "#   - master  --> the spark master node IP:PORT address    \n",
    "#   - appName --> the spark application name \n",
    "#   - config  --> a set of configuration parameters\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .appName(\"Test spark application\")\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea73c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the newly created spark session\n",
    "# this is the entry point to all other Spark functionalities\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1bcfb",
   "metadata": {},
   "source": [
    "The __Spark UI__ link provided won't work, as it refers to the Docker container running the Spark application.\n",
    "\n",
    "Port mapping is however provided for the `4040` port and you can open the Spark UI link to the application pointing your browser to `localhost:4040`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da95662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the Spark Session we can access the Spark Context\n",
    "# the Spark Context is the driver we will use to submit applications to Spark\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5d7a77",
   "metadata": {},
   "source": [
    "## Testing basic parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribute a simple list over the cluster, and counts the elements in parallel\n",
    "sc.parallelize([1,2,3,4,5,6,7,8]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f03dad6",
   "metadata": {},
   "source": [
    "##Â Accessing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab72fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an RDD by reading an input file\n",
    "rdd = sc.textFile(\"../../datasets/lecture1/file_1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b46a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 2 elements from the dataset\n",
    "# `take` is the equivalent of `show`\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe1448b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an RDD by reading ALL files in the folder\n",
    "rdd = sc.textFile(\"/mapd-workspace/datasets/lecture1/file_*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of elements in the RDD\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc5f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many partitions Spark has used to subdivide the dataset\n",
    "rdd.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

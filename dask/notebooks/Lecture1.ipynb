{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of parallel and distributed computing with DASK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap on Task Parallelization (this time using Dask)\n",
    "\n",
    "By now, we should be quite familiar with the programming pattern discussed in previous lectures, where we explored Hadoop MapReduce and the Spark framework.\n",
    "\n",
    "Apart from the specifics of the API, which vary depending on the framework implementation, and the internal workings of job scheduling and resource management, most distributed computing frameworks and tools offer ways to parallelize our tasks through graph schedulers and task optimizers.\n",
    "\n",
    "The workflow remains largely consistent, regardless of the tool used:\n",
    "- **Subdivide large datasets that won't fit into memory into smaller subsets (partitions)**\n",
    "- **Design the entire data processing pipeline before execution and optimize it by breaking it into smaller tasks**\n",
    "- **Schedule and distribute processing close to the data, minimizing data movement (data locality)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Dask, we have access to a variety of dataset representations:\n",
    "\n",
    "- Dask bag (roughly equivalent to the Spark RDD)\n",
    "- Dask array\n",
    "- Dask dataframe (similar to the Spark DataFrame)\n",
    "\n",
    "However, it's important to note that Dask primarily serves as a **scheduler**, written in Python as opposed to Scala \\[*\\], which allows for the lazy execution of Python-like code, distributing it across multiple workers.\n",
    "\n",
    "_\\[*\\] Please note that the fact that Dask's scheduler is written in Python does not necessarily make it better than Spark for most purposes. Both Python and Scala/Java have their own strengths and weaknesses. For instance, due to Python's memory management approach, Dask still faces challenges with unmanaged memory over the lifecycle of an application. For more information, you can refer to this [link](https://coiled.io/blog/tackling-unmanaged-memory-with-dask/)._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting a cluster\n",
    "\n",
    "We can use Dask locally to leverage the multitasking/processing capabilities of our local machine, or we can set up a cluster and deploy a scheduler and multiple worker nodes.\n",
    "\n",
    "Once the cluster is set up, we can initialize a `Client` by providing it with the address of a scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dask distributed client\n",
    "from dask.distributed import Client\n",
    "\n",
    "# instantiate the client by providing \n",
    "# the address:port of the scheduler\n",
    "client = Client('dask-scheduler:8786')\n",
    "\n",
    "# inspect the client\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the status of the cluster with a dedicated webui by accessing the location (IP address) of the scheduler/master node at port `8787`.\n",
    "Using Docker with the appropriate port-mapping (provided in the Docker compose file) we can access the dashboard on `localhost:8787`.\n",
    "\n",
    "The dashboard provides an overview of both the status of the workers, and the execution of the DAG task graphs (when we begin using them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple parallelization of python functions\n",
    "\n",
    "_In the following, we will follow a similar approach to the one provided by the excellent Dask documentation._\n",
    "\n",
    "The simplest example of parallelizing any arbitrary Python code in Dask can be demonstrated with a couple of simple operations, represented by two functions and an arbitrary sleep time of 1 second.\n",
    "\n",
    "_The sleep time is meant to represent arbitrarily complex code (___your___ task) and the time required for its execution._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "# dummy function incrementing the input value by 1\n",
    "def increment(x):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "    # given the input x, return x+1\n",
    "    return x + 1\n",
    "\n",
    "# dummy function decrementing the input value by 1\n",
    "def decrement(x):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "    # given the input x, return x-1\n",
    "    return x - 1\n",
    "\n",
    "# dummy function summing two input values\n",
    "def add(x, y):\n",
    "    # sleep for 1s\n",
    "    sleep(1)\n",
    "    # given the inputs x and y, return x+y\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are purely Python functions...\n",
    "\n",
    "We can test the functions locally by running them on the client (not on the cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# we should expect a wall-time of roughly 3 seconds (3x 1sec sleep)\n",
    "x = increment(1)\n",
    "y = decrement(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for Dask to leverage the processing units assigned to the cluster, we need to construct the Directed Acyclic Graph (DAG) corresponding to the execution of the code we want to deploy on the cluster. Subsequently, we allow the scheduler to dispatch the tasks to the workers.\n",
    "\n",
    "This is accomplished in Dask by marking a function as `delayed`.\n",
    "\n",
    "The `delayed` method in Dask accepts two main arguments:\n",
    "- The first argument is the function to be executed in parallel.\n",
    "- The subsequent arguments are the arguments upon which the original function will operate.\n",
    "\n",
    "Now, we aim to transform the `increment`, `decrement`, and `add` functions, thereby making them **lazy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dask delayed module\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# make the function behave as lazy with delayed\n",
    "#\n",
    "# result = delayed(your_function)(<your_function_arguments>)\n",
    "x = delayed(increment)(1)\n",
    "y = delayed(decrement)(2)\n",
    "z = delayed(add)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, as is typical with lazy operations, the results are not yet stored in `z`.\n",
    "\n",
    "At this stage, `z` simply represents the \"plan\" of the code execution, created by the Directed Acyclic Graph (DAG) task scheduler.\n",
    "\n",
    "To visualize the plan of execution, we can utilize the `visualize()` method (note that the `graphviz` Python package is required for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask offers a nice and simple \n",
    "# visualization of the DAG\n",
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization represents the task graph **before** any optimization is applied.\n",
    "\n",
    "We can also request Dask to provide the DAG **after** optimization by using the `optimize_graph=True` option.\n",
    "\n",
    "For this simple task, we shouldn't anticipate significant optimization. However, with more complex tasks, we can expect to see a noticeable difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized DAG\n",
    "z.visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, Dask also offers a high-level task visualization tool that can be accessed by clicking on it from within a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high-level task graph\n",
    "z.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually execute the job, we need to instruct Dask to trigger the execution by requesting the results of the lazy operation with the `compute()` method.\n",
    "\n",
    "Under the hood, Dask will send the computational graph to the scheduler and dispatch the tasks to the workers, similar to what was discussed for Spark.\n",
    "\n",
    "It's important to note that `compute()` is a blocking operation, meaning that the program will wait for the results to be computed before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# the execution time should now be \n",
    "# less than the 3 seconds measured above\n",
    "# \n",
    "# ideally, we should expect 2 seconds:\n",
    "#  - 1 second (sleep time) to run the increment and decrement functions --in parallel--\n",
    "#  - 1 second (sleep time) to run the add function based on the results of the previous stages\n",
    "z.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Futures and Eager computation with `submit`\n",
    "\n",
    "The eager operation alternative to `delayed` in Dask is `submit`.\n",
    "\n",
    "Using `submit` instructs Dask to immediately submit our task to the cluster and begin executing it on the available computing resources.\n",
    "\n",
    "This approach is quite similar, and in fact, almost equivalent to the concept of a batch system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the function increment with argument 1\n",
    "# return a \"promise\" of getting the actual result\n",
    "future = client.submit(increment, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `increment` with its argument `1` is submitted to the cluster, and the `future` object is returned immediately. \n",
    "However, this `future` object **does not contain the result of the computation**; rather, it represents an execution **promise** of the instruction that was submitted. \n",
    "It's important to note that the execution may not have been completed yet, as the cluster might take some time to process it.\n",
    "\n",
    "The `future` variable **does not contain the result** itself, but rather a promise of it for when the computation is completed. \n",
    "\n",
    "**The result of the computation will remain on the worker nodes of our cluster and will not be sent back to our client until we explicitly request it.**\n",
    "\n",
    "This concept is particularly useful when dealing with large computation results or when subsequent tasks need to be executed on the data resulting from previous tasks.\n",
    "\n",
    "To retrieve the result from the cluster to the client, we can invoke the `gather` function on the `future` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the promise of the result\n",
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result, gathered from the cluster\n",
    "client.gather(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach can be extremely useful in situations where we need to submit a task multiple times, potentially with different input parameters. \n",
    "\n",
    "This scenario is common in machine learning, especially when optimizing an algorithm across a hyperparameter space for a specific dataset.\n",
    "\n",
    "The concept behind this approach allows us to `map` the instruction we want to execute to each element of a dataset, effectively submitting the same operation across the entire dataset.\n",
    "\n",
    "_In practical terms, we are parallelizing our function in a manner similar to how it's done with the Python `multiprocessing` module, but leveraging a large and horizontally scalable set of computing resources._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart from a list of integers\n",
    "data = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# submit the increment function on ALL elements of the list\n",
    "# using a `map` approach (each element is independent from the others)\n",
    "future_results = client.map(increment, data)\n",
    "\n",
    "# this execution is Eager and Asynchronous  \n",
    "future_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the status of the jobs\n",
    "future_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the data from the cluster\n",
    "new_data = client.gather(future_results)\n",
    "\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`client.map` operates **asynchronously**, meaning that the cluster computes the results without blocking our local Python code for task completion.\n",
    "\n",
    "Think of it in terms of `process.start()` and `process.join()` in multiprocessing.\n",
    "\n",
    "However, **if** we need to wait for the result of a `submit` computation to be ready — for instance, if we require it as an input for other computations — we can use the `wait` method. This method blocks the execution of new code and waits for the computation of the future.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dask wait \n",
    "from dask.distributed import wait\n",
    "\n",
    "# start the computation\n",
    "new_future = client.map(increment, new_data)\n",
    "\n",
    "# block this python process \n",
    "# and wait for the remote task to be completed\n",
    "wait(new_future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result, gathered from the cluster\n",
    "client.gather(new_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In complete analogy to what we have discussed with the `delayed` lazy execution, we can combine multiple instructions that need to be submitted to the cluster to create a more complex job to run on our cluster.\n",
    "\n",
    "It's important to remember that the results of the `submit` execution reside on the cluster until a `gather` is used.\n",
    "\n",
    "This means that **we can submit a task that takes as an argument an execution promise** of an instruction that has been previously submitted*.\n",
    "\n",
    "The `gather` function should be invoked at the end of the program, only when the results need to be effectively retrieved from the cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit the increment function\n",
    "x = client.submit(increment, 1)\n",
    "# submit the decrement function\n",
    "y = client.submit(decrement, 2)\n",
    "\n",
    "# submit the add function\n",
    "#\n",
    "# this will run on the promises of the results \n",
    "# of both the x and y (possibly not yet available)\n",
    "total = client.submit(add, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is still the execution promise\n",
    "print(total)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the final result\n",
    "client.gather(total)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Parallelization of a for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, with the previous knowledge of what discussed with Spark, and the `delayed` and `compute` Dask operations, we should already be able to run simple \"dummy\" tasks.\n",
    "\n",
    "Starting from a list $\\vec{x}$ of values:\n",
    "1. write a function to increment each element $x_i$ by a random value $\\delta x_i$ (in the 0-1 range)\n",
    "2. write a function to multiply the resulting value by 10\n",
    "3. loop over $\\vec{x}$ and apply both functions on each $x_i$ element using the `delayed`\n",
    "4. retrieve the sum of all the new updated elements\n",
    "5. visualize the task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# input data\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# 1. increment function\n",
    "def add_rand(x):\n",
    "    return x+random.random()\n",
    "\n",
    "# 2. multiplication function\n",
    "def mult_ten(x):\n",
    "    return x*10\n",
    "\n",
    "# placeholder for the updated array\n",
    "results = []\n",
    "\n",
    "# 3. for each element\n",
    "#  - add a random value\n",
    "#  - multiply by ten\n",
    "#  - append the new data in a list\n",
    "for x in data:\n",
    "\n",
    "\n",
    "    \n",
    "# 4. sum all elements of the list\n",
    "total = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. visualize the task graph\n",
    "total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the Optimized task graph\n",
    "# \n",
    "# once again, we should not expect any real optimization \n",
    "# to be viable for this map-like operations\n",
    "total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# compute the result and time it\n",
    "result = total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check the result\n",
    "print(\"result: \",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the same task with the eager `submit` execution approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# placeholder for the updated array\n",
    "results = []\n",
    "\n",
    "# for each element\n",
    "#  - add a random value\n",
    "#  - multiply by ten\n",
    "#  - append the new data in a list\n",
    "for x in data:\n",
    "\n",
    "\n",
    "    \n",
    "# sum of all elements of the list\n",
    "total = client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the future object\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result\n",
    "print(\"result: \",client.     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly another alternative to the previous approach using the **map** Dask functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "# map both functions on all data elements\n",
    "\n",
    "\n",
    "# submit the sum function on the z future\n",
    "total = client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the future object\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result\n",
    "print(\"result: \",client.    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Map Reduce with Dask\n",
    "\n",
    "So far, we have primarily used _map_-like operations and then collected all outputs in a single `sum` operation.\n",
    "\n",
    "Now, we need to explore how we could write in Dask an equivalent _reduce_ function to evaluate the sum of a list of elements pair-wise.\n",
    "\n",
    "We can utilize the `add` function previously defined, which includes a 1-second sleep time, to visualize the time taken to run the task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is a pair-reduction algorithm implemented with a (_\"nasty\"_) nested `for` loop and some simple Python logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "L = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "while len(L) > 1:\n",
    "    # Temporary list to store the sum of pairs\n",
    "    _ = []  \n",
    "    \n",
    "    # Iterate over the indices of L with a step of 2 to process pairs of elements    \n",
    "    for i in range(0, len(L), 2):\n",
    "        if i+1 < len(L):\n",
    "            # Sum pairs of elements in the list\n",
    "            pair_sum = add(L[i], L[i + 1]) \n",
    "        else:\n",
    "            # If the length of L is odd, add the last element with 0\n",
    "            pair_sum = add(L[i], 0)       \n",
    "        _.append(pair_sum)  # Append the sum to the temporary list\n",
    "    \n",
    "    # Replace L with the temporary list containing pair sums\n",
    "    L = _  \n",
    "\n",
    "# Print the final result\n",
    "print(\"result:\",L[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize this reduce task, we can define the pair-wise sum of neighbor elements as `delayed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "L = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# rewrite the same algorithm using delayed\n",
    "while len(L) > 1:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the task graph for L[0]\n",
    "L[0].visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# compute the result\n",
    "result = L[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"result\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Count how many words are present in a series of documents\n",
    "\n",
    "Given a dataset composed of pieces of text taken from `sklearn`, the task is to count how many words are present in each document and calculate the total number of words in the dataset.\n",
    "\n",
    "The dataset consists of approximately 8000 documents.\n",
    "\n",
    "While one way to proceed would be to loop over all documents and count the words in each, a more efficient approach is needed, and we should now know how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from dask.distributed import Client\n",
    "import time\n",
    "\n",
    "categories = [\n",
    "     'comp.graphics',\n",
    "     'comp.os.ms-windows.misc',\n",
    "     'comp.sys.ibm.pc.hardware',\n",
    "     'comp.sys.mac.hardware',\n",
    "     'comp.windows.x',\n",
    "     'misc.forsale',\n",
    "     'rec.autos',\n",
    "     'rec.motorcycles',\n",
    "     'rec.sport.baseball',\n",
    "     'rec.sport.hockey',\n",
    "     'sci.crypt',\n",
    "     'sci.electronics',\n",
    "     'sci.med',\n",
    "     'sci.space'\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(subset='train', categories=categories ).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Documents in the dataset:\",len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple function to split a body of text \n",
    "# into words and count them\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple example of single-threaded execution in plain Python can be the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize a word-per-document list\n",
    "total_words_per_document = []\n",
    "\n",
    "# count the number of words in each document\n",
    "for document in dataset:\n",
    "    total_words_per_document.append(count_words(document))\n",
    "\n",
    "# calculate the total number of words in the dataset\n",
    "total_words_dataset = sum(total_words_per_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of words in the dataset: {total_words_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(total_words_per_document,bins=range(0,1000,10));\n",
    "plt.xlabel('words per document');\n",
    "plt.ylabel('counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the distributed version using the `delayed` lazy execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# initialize a word-per-document list\n",
    "\n",
    "\n",
    "# count the number of words in each document\n",
    "# using dask delayed \n",
    "\n",
    "\n",
    "\n",
    "# calculate the total number of words in the dataset\n",
    "# using dask delayed \n",
    "total_words_lazy = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# execute the tasks and retrieve the result\n",
    "lazy_result = \n",
    "\n",
    "print(f\"Total number of words in the dataset: {lazy_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the distributed version using the `map` and `submit` eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# count the number of words in each document\n",
    "# using dask map \n",
    "\n",
    "\n",
    "\n",
    "# calculate the total number of words in the dataset\n",
    "# using dask submit \n",
    "total_words_eager = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the future object\n",
    "total_words_eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eager_result = \n",
    "print(\"Total number of words in the dataset: {}\".format(eager_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Calculate the first $n$ Fibonacci numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the plain Python algorithm to evaluate the sequence of Fibonacci up to the $n$-th element:\n",
    "\n",
    "Once again, let's start from some simple single-threaded Python code for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fibonacci_sequential(num):\n",
    "    iteration = 1\n",
    "    fibonacci = []\n",
    "    if num <= 0:\n",
    "        pass\n",
    "    elif num == 1:\n",
    "        fibonacci.append(1)\n",
    "    elif num == 2:\n",
    "        fibonacci.append(1)\n",
    "        fibonacci.append(1)\n",
    "    elif num > 2:\n",
    "        fibonacci.append(1)\n",
    "        fibonacci.append(1)\n",
    "        while iteration < (num - 1):\n",
    "            fibonacci.append(fibonacci[iteration] + fibonacci[iteration-1])\n",
    "            iteration+=1\n",
    "    return fibonacci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "n = 17\n",
    "\n",
    "print(f\"The first {n} fibonacci numbers are: {fibonacci_sequential(n)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the same algorithm using the Dask lazy evaluation, with `delayed`, and inspect the task graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to generalize the append we need \n",
    "# to create a function that does not get \n",
    "# an error when called with an empty array\n",
    "def append(arr = [], val = 0):\n",
    "    if val != None:\n",
    "        arr.append(val)\n",
    "    return arr\n",
    "\n",
    "# define the delayed version of the fibonacci code\n",
    "def fibonacci_delayed(num):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it\n",
    "result = fibonacci_delayed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the graph\n",
    "result.visualize(rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Monte Carlo Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we want to integrate of a function via MonteCarlo technique, as you have discussed in LCP Module A.\n",
    "\n",
    "Let's use the function $$f(x) =\\sin^2{\\left(\\frac{1}{x(2-x)}\\right)}$$ and let's integrate in the range $(0,2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    return (np.sin(1/(x*(2-x))))**2\n",
    "\n",
    "x=np.linspace(-0.2,2.2,4000)\n",
    "\n",
    "plt.figure(figsize=(16,6));\n",
    "plt.plot(x,f(x),'grey','.');\n",
    "plt.fill_between(x[np.where((x>0) & (x<2))],[1]*len(np.where((x>0) & (x<2))), alpha=0.2);\n",
    "plt.fill_between(x[np.where((x>0) & (x<2))],f(x[np.where((x>0) & (x<2))]), alpha=0.2);\n",
    "plt.vlines([0, 2], 0, 1, colors = [\"k\", \"k\"], linestyles = [\"dashed\", \"dashed\"],linewidths=[3,3],zorder=20);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the single-threaded Python code to execute this task, and evaluate the integral over N=100 000 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Monte Carlo integration using Python's built-in random module\n",
    "\n",
    "# number of points to use for integration\n",
    "N = 100000\n",
    "\n",
    "# list of counts for points that pass the function test\n",
    "count = []\n",
    "\n",
    "# function to check if a random point is passing\n",
    "def pass_function():\n",
    "    # generate a random x value between 0 and 2\n",
    "    # generate a random y value between 0 and 1\n",
    "    x = 2 * random.random()\n",
    "    y = random.random()\n",
    "    # check if the point (x, y) is under the curve defined by f(x)\n",
    "    return 1 if y < f(x) else 0\n",
    "\n",
    "# iterate over all points and count how many pass the function test\n",
    "for i in range(N):\n",
    "    count.append(pass_function())\n",
    "\n",
    "# compute the integral by dividing the sum of the counts by the total number of points,\n",
    "# and multiply by the width of the integration range (2)\n",
    "I = 2 * sum(count) / N\n",
    "\n",
    "# print the result\n",
    "print(f\"Integral = {I}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply the minimal changes to the code to deploy the same integral computation on the Dask cluster using `delayed`\n",
    "\n",
    "**NOTE**: Do NOT use 100 000 points in this case, but ___limit the computation to N=1 000 points___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Monte Carlo integration using Python's built-in random module\n",
    "\n",
    "# number of points to use for integration\n",
    "N = 1_000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use Python **decorators** to declare that an entire function is going to be interpreted as `delayed`\n",
    "\n",
    "```python\n",
    "@dask.delayed\n",
    "def my_delayed_function(arg):\n",
    "    ...\n",
    "    return val\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Monte Carlo integration using Python's built-in random module\n",
    "\n",
    "# import dask to use its delayed decorator\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are we experiencing degraded performance compared to single-threaded execution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask code may exhibit slower performance than the equivalent plain Python code due to the **overhead** introduced by Dask itself.\n",
    "\n",
    "When utilizing Dask, we construct a task graph that Dask must traverse to execute the computation. This process involves additional computation and communication overhead between the scheduler and the workers, contributing to increased execution time compared to the plain Python version.\n",
    "\n",
    "_Each delayed task inherently incurs an overhead of hundreds of microseconds. With a large number of computations issued, this overhead can become significant._\n",
    "\n",
    "Furthermore, our usage of `delayed` to parallelize the computation may not fully exploit Dask's parallelism capabilities. By creating a list of `delayed` objects and then computing them all at once with `delayed(sum)(count)`, we essentially execute 1000 `count` functions as individual processes on the worker nodes.\n",
    "\n",
    "This approach can be inefficient since it still processes the loop sequentially before aggregating the results in parallel. A more effective strategy could involve leveraging Dask's `dask.bag` to parallelize the computation more efficiently.\n",
    "\n",
    "Overall, it's crucial to ensure that we maximize Dask's capabilities to parallelize the computation effectively. In some scenarios, utilizing Dask may not yield any performance benefits and could potentially be slower than plain Python if the task is not optimized for parallel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: $\\pi$ via MonteCarlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-run the evaluation of $\\pi$ using the Monte Carlo technique, similar to what was done during the Spark hands-on session.\n",
    "\n",
    "However, it's important to note that, at present, we haven't partitioned the data in Dask (yet).\n",
    "\n",
    "Instead, we're instructing Dask to execute a simple task for each entry in our list, which incurs significant overhead.\n",
    "\n",
    "Therefore, it's advisable to start with a very limited number of points, approximately 10, and gradually increase to a maximum of around 100. \n",
    "This cautious approach allows us to monitor the job's status from the dashboard and assess its performance effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "# set the number of points to use\n",
    "num_points = 100\n",
    "\n",
    "# list to hold the points inside the circle\n",
    "points_in_circle = []\n",
    "\n",
    "# function to check if a point is inside the circle\n",
    "def in_circle():\n",
    "    # generate a random x and y value between 0 and 1\n",
    "\n",
    "    \n",
    "# iterate over all the points and count how many are inside the circle\n",
    "\n",
    "\n",
    "# count the number of points inside the circle\n",
    "num_points_inside = \n",
    "\n",
    "# compute pi\n",
    "pi = 4*num_points_inside/num_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result\n",
    "print (f\"{pi=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `delayed` to parallelize the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result\n",
    "print (f\"{pi=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `map` or `submit` to parallelize the computation using the Dask eager execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `submit` we have to be very careful...\n",
    "\n",
    "Naively calling `client.map` with a function that returns a random variable may result in the same exact random value being returned every time. Essentially, we have _one function_ executed 10000 times.\n",
    "\n",
    "However, we can inform Dask that this function is `impure`, meaning it can produce different output values for the same input:\n",
    "\n",
    "```python\n",
    "client.submit(your_function, pure=False)\n",
    "```\n",
    "\n",
    "This approach ensures that we obtain 10000 different random variables. However, it's important to note that while this guarantees diversity in the random variables, it can pose challenges for Dask's parallelization. We may encounter unnecessary overhead and performance penalties due to the increased complexity introduced by impure functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print result\n",
    "print (f\"{pi=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-away Message\n",
    "\n",
    "Dask offers flexibility in scheduling our Pythonic code and tasks, allowing for both lazy and eager execution strategies.\n",
    "\n",
    "However, as discussed, parallelization tends to be most beneficial for tasks involving substantial data or computationally intensive operations that can be divided into smaller chunks and executed concurrently.\n",
    "\n",
    "The simple functions used in our examples do not represent computationally \"heavy\" tasks well, and thus, the benefits of parallelization (if any at all) may not be fully realized.\n",
    "\n",
    "Moreover, Dask Distributed can yield significant performance gains when computations are distributed across multiple nodes or machines, rather than just across a few CPU cores on a single machine (as in our case using Docker compose).\n",
    "\n",
    "Given these factors, it's always essential to thoroughly evaluate the performance of parallelized code and compare it against single-threaded implementations to ensure that the desired efficiency gains are achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use `docker compose down` to stop and clear all running containers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
